---
id: week-12-conversational-ai
title: Week 12 â€“ Conversational AI Integration for Humanoid Robots
---

# Week 12: Conversational AI Integration for Humanoid Robots

## Learning Objectives

By the end of this week, students will be able to:
- Integrate speech recognition and natural language understanding in humanoid robots
- Design conversational interfaces that work with robotic action planning
- Implement multimodal interaction combining speech, gesture, and vision
- Create dialogue management systems for human-robot interaction
- Evaluate the effectiveness of conversational AI systems in robotics

## Introduction to Conversational AI for Robotics

Conversational AI enables humanoid robots to interact naturally with humans using speech and natural language. This integration requires combining multiple technologies: speech recognition, natural language processing, dialogue management, and action execution.

### Architecture of Conversational AI Systems

A typical conversational AI system for humanoid robots includes:

1. **Speech Recognition**: Converting speech to text
2. **Natural Language Understanding (NLU)**: Interpreting user intent
3. **Dialogue Management**: Maintaining conversation context
4. **Natural Language Generation (NLG)**: Creating robot responses
5. **Speech Synthesis**: Converting text to speech
6. **Action Execution**: Performing robot actions based on commands

## Speech Recognition Integration

Speech recognition is the first step in enabling voice interaction with humanoid robots:

### Using OpenAI Whisper for Speech Recognition

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from sensor_msgs.msg import AudioData
import whisper
import torch
import numpy as np
import pyaudio
import wave
import threading
import queue

class SpeechRecognitionNode(Node):
    def __init__(self):
        super().__init__('speech_recognition_node')
        
        # Initialize Whisper model
        self.get_logger().info("Loading Whisper model...")
        self.model = whisper.load_model("base")  # Use "small" or "medium" for better accuracy
        self.get_logger().info("Whisper model loaded")
        
        # Audio input settings
        self.chunk = 1024
        self.format = pyaudio.paInt16
        self.channels = 1
        self.rate = 16000
        self.record_seconds = 5
        
        # Publisher for recognized text
        self.text_pub = self.create_publisher(
            String,
            '/recognized_speech',
            10
        )
        
        # Initialize audio stream
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(
            format=self.format,
            channels=self.channels,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk
        )
        
        # Start audio recording thread
        self.recording = True
        self.audio_queue = queue.Queue()
        self.recording_thread = threading.Thread(target=self.record_audio)
        self.recording_thread.start()
        
        # Timer for processing audio
        self.process_timer = self.create_timer(5.0, self.process_audio)
        
        self.get_logger().info("Speech recognition node initialized")
    
    def record_audio(self):
        """Continuously record audio to queue"""
        while self.recording:
            data = self.stream.read(self.chunk)
            self.audio_queue.put(data)
    
    def process_audio(self):
        """Process accumulated audio and run speech recognition"""
        if self.audio_queue.empty():
            return
            
        # Collect audio frames
        frames = []
        while not self.audio_queue.empty():
            frames.append(self.audio_queue.get())
        
        if len(frames) == 0:
            return
            
        # Convert to numpy array
        audio_data = b''.join(frames)
        audio_np = np.frombuffer(audio_data, dtype=np.int16)
        
        # Normalize audio to [-1, 1]
        audio_float = audio_np.astype(np.float32) / 32768.0
        
        # Run speech recognition
        try:
            result = self.model.transcribe(audio_float)
            recognized_text = result['text'].strip()
            
            if recognized_text:  # Only publish if there's actual text
                self.get_logger().info(f"Recognized: {recognized_text}")
                
                # Publish recognized text
                text_msg = String()
                text_msg.data = recognized_text
                self.text_pub.publish(text_msg)
        
        except Exception as e:
            self.get_logger().error(f"Speech recognition error: {str(e)}")
    
    def destroy_node(self):
        """Clean up audio resources"""
        self.recording = False
        if hasattr(self, 'stream'):
            self.stream.stop_stream()
            self.stream.close()
        if hasattr(self, 'audio'):
            self.audio.terminate()
        super().destroy_node()

# Alternative implementation using ROS 2 audio input
class ROSAudioSpeechRecognitionNode(Node):
    def __init__(self):
        super().__init__('ros_audio_speech_recognition')
        
        # Initialize Whisper model
        self.model = whisper.load_model("base")
        
        # Subscribe to audio data
        self.audio_sub = self.create_subscription(
            AudioData,
            '/audio_input',
            self.audio_callback,
            10
        )
        
        # Publisher for recognized text
        self.text_pub = self.create_publisher(
            String,
            '/recognized_speech',
            10
        )
    
    def audio_callback(self, msg):
        """Process audio data from ROS topic"""
        # Convert audio data to numpy array
        audio_np = np.frombuffer(msg.data, dtype=np.int8).astype(np.float32)
        
        # Normalize audio to [-1, 1]
        audio_normalized = audio_np / 128.0
        
        # Run speech recognition
        try:
            result = self.model.transcribe(audio_normalized)
            recognized_text = result['text'].strip()
            
            if recognized_text:
                self.get_logger().info(f"Recognized: {recognized_text}")
                
                # Publish recognized text
                text_msg = String()
                text_msg.data = recognized_text
                self.text_pub.publish(text_msg)
        
        except Exception as e:
            self.get_logger().error(f"Speech recognition error: {str(e)}")
```

## Natural Language Understanding

Natural Language Understanding (NLU) interprets the user's intent from recognized text:

### Intent Classification and Entity Extraction

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from std_msgs.msg import Bool
import json
import re
from typing import Dict, List, Tuple

class NaturalLanguageUnderstandingNode(Node):
    def __init__(self):
        super().__init__('nlu_node')
        
        # Subscribe to recognized speech
        self.speech_sub = self.create_subscription(
            String,
            '/recognized_speech',
            self.speech_callback,
            10
        )
        
        # Publishers for understanding results
        self.intent_pub = self.create_publisher(
            String,
            '/intent',
            10
        )
        
        self.action_plan_pub = self.create_publisher(
            String,
            '/action_plan',
            10
        )
        
        # Define supported intents
        self.intents = {
            'move': {
                'patterns': [
                    r'go to (.+)',
                    r'walk to (.+)',
                    r'move to (.+)',
                    r'go (.+)',
                    r'walk (.+)',
                    r'move (.+)'
                ],
                'entities': ['location']
            },
            'grasp': {
                'patterns': [
                    r'pick up (.+)',
                    r'grasp (.+)',
                    r'grab (.+)',
                    r'get (.+)',
                    r'take (.+)'
                ],
                'entities': ['object']
            },
            'place': {
                'patterns': [
                    r'put (.+) on (.+)',
                    r'place (.+) on (.+)',
                    r'place (.+) at (.+)'
                ],
                'entities': ['object', 'location']
            },
            'greet': {
                'patterns': [
                    r'hello',
                    r'hi',
                    r'hey',
                    r'good morning',
                    r'good afternoon',
                    r'good evening'
                ],
                'entities': []
            },
            'stop': {
                'patterns': [
                    r'stop',
                    r'pause',
                    r'wait',
                    r'freeze'
                ],
                'entities': []
            }
        }
        
        self.get_logger().info("NLU node initialized with intents")
    
    def speech_callback(self, msg):
        """Process recognized speech and extract intent"""
        text = msg.data.lower().strip()
        self.get_logger().info(f"Processing: {text}")
        
        # Classify intent and extract entities
        intent, entities = self.classify_intent(text)
        
        if intent:
            self.get_logger().info(f"Detected intent: {intent}, entities: {entities}")
            
            # Create intent message
            intent_msg = String()
            intent_msg.data = json.dumps({
                'intent': intent,
                'entities': entities,
                'original_text': msg.data
            })
            self.intent_pub.publish(intent_msg)
            
            # Generate action plan based on intent
            action_plan = self.generate_action_plan(intent, entities)
            if action_plan:
                action_msg = String()
                action_msg.data = json.dumps(action_plan)
                self.action_plan_pub.publish(action_msg)
        else:
            self.get_logger().warn(f"No intent detected for: {text}")
    
    def classify_intent(self, text: str) -> Tuple[str, Dict[str, str]]:
        """Classify the intent of the input text and extract entities"""
        for intent_name, intent_info in self.intents.items():
            for pattern in intent_info['patterns']:
                match = re.search(pattern, text)
                if match:
                    # Extract entities based on pattern groups
                    entities = {}
                    if intent_info['entities']:
                        for i, entity_name in enumerate(intent_info['entities']):
                            if i < len(match.groups()):
                                entities[entity_name] = match.group(i + 1).strip()
                    
                    return intent_name, entities
        
        return None, {}
    
    def generate_action_plan(self, intent: str, entities: Dict[str, str]) -> Dict:
        """Generate an action plan based on intent and entities"""
        if intent == 'move':
            location = entities.get('location', 'unknown')
            return {
                'actions': [
                    {
                        'type': 'navigate',
                        'target': location,
                        'description': f'Navigate to {location}'
                    }
                ]
            }
        
        elif intent == 'grasp':
            obj = entities.get('object', 'unknown object')
            return {
                'actions': [
                    {
                        'type': 'find_object',
                        'target': obj,
                        'description': f'Find {obj}'
                    },
                    {
                        'type': 'approach_object',
                        'target': obj,
                        'description': f'Approach {obj}'
                    },
                    {
                        'type': 'grasp_object',
                        'target': obj,
                        'description': f'Grasp {obj}'
                    }
                ]
            }
        
        elif intent == 'place':
            obj = entities.get('object', 'unknown object')
            location = entities.get('location', 'unknown location')
            return {
                'actions': [
                    {
                        'type': 'navigate',
                        'target': location,
                        'description': f'Navigate to {location}'
                    },
                    {
                        'type': 'place_object',
                        'object': obj,
                        'location': location,
                        'description': f'Place {obj} at {location}'
                    }
                ]
            }
        
        elif intent == 'greet':
            return {
                'actions': [
                    {
                        'type': 'greet',
                        'description': 'Greet the user'
                    }
                ]
            }
        
        elif intent == 'stop':
            return {
                'actions': [
                    {
                        'type': 'stop',
                        'description': 'Stop current action'
                    }
                ]
            }
        
        return None

class DialogueManagerNode(Node):
    def __init__(self):
        super().__init__('dialogue_manager')
        
        # Subscribe to intents and action plans
        self.intent_sub = self.create_subscription(
            String,
            '/intent',
            self.intent_callback,
            10
        )
        
        self.action_plan_sub = self.create_subscription(
            String,
            '/action_plan',
            self.action_plan_callback,
            10
        )
        
        # Publisher for robot responses
        self.response_pub = self.create_publisher(
            String,
            '/robot_response',
            10
        )
        
        # Publisher for actions to execute
        self.action_pub = self.create_publisher(
            String,
            '/robot_action',
            10
        )
        
        # Maintain conversation context
        self.conversation_context = {
            'last_intent': None,
            'pending_actions': [],
            'object_references': {},
            'location_references': {}
        }
    
    def intent_callback(self, msg):
        """Handle intent recognition"""
        try:
            intent_data = json.loads(msg.data)
            intent = intent_data['intent']
            entities = intent_data['entities']
            
            # Update conversation context
            self.conversation_context['last_intent'] = intent
            
            # Generate appropriate response based on intent
            response = self.generate_response(intent, entities)
            
            if response:
                response_msg = String()
                response_msg.data = response
                self.response_pub.publish(response_msg)
        
        except json.JSONDecodeError:
            self.get_logger().error("Invalid JSON in intent message")
    
    def action_plan_callback(self, msg):
        """Handle action plan"""
        try:
            action_plan = json.loads(msg.data)
            
            # Publish actions to execute
            for action in action_plan['actions']:
                action_msg = String()
                action_msg.data = json.dumps(action)
                self.action_pub.publish(action_msg)
        
        except json.JSONDecodeError:
            self.get_logger().error("Invalid JSON in action plan message")
    
    def generate_response(self, intent: str, entities: Dict[str, str]) -> str:
        """Generate appropriate response based on intent"""
        if intent == 'greet':
            return "Hello! How can I assist you today?"
        
        elif intent == 'move':
            location = entities.get('location', 'that location')
            return f"OK, I will move to {location}."
        
        elif intent == 'grasp':
            obj = entities.get('object', 'that object')
            return f"OK, I will pick up {obj}."
        
        elif intent == 'place':
            obj = entities.get('object', 'the object')
            location = entities.get('location', 'that location')
            return f"OK, I will place {obj} at {location}."
        
        elif intent == 'stop':
            return "I will stop my current action."
        
        else:
            return "I understand. Let me process that request."
```

## Multimodal Interaction

Effective conversational AI for humanoid robots combines multiple modalities:

### Vision-Language Integration

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import String
from cv_bridge import CvBridge
import torch
import clip
from PIL import Image as PILImage
import numpy as np

class VisionLanguageNode(Node):
    def __init__(self):
        super().__init__('vision_language_node')
        
        # Load CLIP model for vision-language tasks
        self.clip_model, self.clip_preprocess = clip.load("ViT-B/32")
        self.clip_model.eval()
        
        # Initialize CV bridge
        self.bridge = CvBridge()
        
        # Subscribe to camera and recognized speech
        self.image_sub = self.create_subscription(
            Image,
            '/camera/rgb/image_raw',
            self.image_callback,
            10
        )
        
        self.speech_sub = self.create_subscription(
            String,
            '/recognized_speech',
            self.speech_callback,
            10
        )
        
        # Publisher for vision-language results
        self.vl_result_pub = self.create_publisher(
            String,
            '/vision_language_result',
            10
        )
        
        # Store latest image and speech for multimodal processing
        self.latest_image = None
        self.latest_speech = None
        
        self.get_logger().info("Vision-Language node initialized")
    
    def image_callback(self, msg):
        """Store latest image for multimodal processing"""
        try:
            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
            self.latest_image = cv_image
        except Exception as e:
            self.get_logger().error(f"Error processing image: {str(e)}")
    
    def speech_callback(self, msg):
        """Process speech with visual context"""
        self.latest_speech = msg.data
        
        # If we have both image and speech, perform multimodal processing
        if self.latest_image is not None:
            self.process_multimodal_input()
    
    def process_multimodal_input(self):
        """Process combined visual and linguistic input"""
        image = self.latest_image
        text = self.latest_speech
        
        # Convert image for CLIP
        pil_image = PILImage.fromarray(image)
        image_input = self.clip_preprocess(pil_image).unsqueeze(0)
        
        # Tokenize text
        text_input = clip.tokenize([text])
        
        # Get similarity
        with torch.no_grad():
            image_features = self.clip_model.encode_image(image_input)
            text_features = self.clip_model.encode_text(text_input)
            
            # Normalize features
            image_features /= image_features.norm(dim=-1, keepdim=True)
            text_features /= text_features.norm(dim=-1, keepdim=True)
            
            # Compute similarity
            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
            similarity_score = similarity[0][0].item()
        
        # Determine if visual context matches speech
        result = {
            'text': text,
            'similarity_score': similarity_score,
            'match_confidence': similarity_score,
            'action_needed': similarity_score > 0.3  # Threshold for action
        }
        
        # Publish result
        result_msg = String()
        result_msg.data = json.dumps(result)
        self.vl_result_pub.publish(result_msg)
        
        self.get_logger().info(f"Vision-language similarity: {similarity_score:.2f}")

class MultimodalControllerNode(Node):
    def __init__(self):
        super().__init__('multimodal_controller')
        
        # Subscribe to multimodal results and intents
        self.vl_result_sub = self.create_subscription(
            String,
            '/vision_language_result',
            self.vl_result_callback,
            10
        )
        
        self.intent_sub = self.create_subscription(
            String,
            '/intent',
            self.intent_callback,
            10
        )
        
        # Publisher for robot actions
        self.action_pub = self.create_publisher(
            String,
            '/robot_action',
            10
        )
        
        # Store multimodal context
        self.multimodal_context = {
            'last_vl_result': None,
            'last_intent': None
        }
    
    def vl_result_callback(self, msg):
        """Handle vision-language results"""
        try:
            vl_result = json.loads(msg.data)
            self.multimodal_context['last_vl_result'] = vl_result
            
            # If we have a matching intent, create a combined action plan
            if self.multimodal_context['last_intent']:
                self.create_multimodal_action_plan()
        except json.JSONDecodeError:
            self.get_logger().error("Invalid JSON in vision-language result")
    
    def intent_callback(self, msg):
        """Handle intent recognition"""
        try:
            intent_data = json.loads(msg.data)
            self.multimodal_context['last_intent'] = intent_data
            
            # If we have a matching vision-language result, create action plan
            if self.multimodal_context['last_vl_result']:
                self.create_multimodal_action_plan()
        except json.JSONDecodeError:
            self.get_logger().error("Invalid JSON in intent message")
    
    def create_multimodal_action_plan(self):
        """Create action plan based on both vision and language input"""
        vl_result = self.multimodal_context['last_vl_result']
        intent_data = self.multimodal_context['last_intent']
        
        intent = intent_data['intent']
        entities = intent_data['entities']
        
        # Enhance action plan with visual information
        if intent == 'grasp' and vl_result['action_needed']:
            # Use visual information to refine grasp action
            obj = entities.get('object', 'unknown object')
            
            action_plan = {
                'actions': [
                    {
                        'type': 'find_object_visual',
                        'target': obj,
                        'visual_context': vl_result,
                        'description': f'Find {obj} using visual context'
                    },
                    {
                        'type': 'approach_object',
                        'target': obj,
                        'description': f'Approach {obj}'
                    },
                    {
                        'type': 'grasp_object',
                        'target': obj,
                        'visual_alignment': True,
                        'description': f'Grasp {obj} with visual alignment'
                    }
                ]
            }
            
            # Publish action plan
            action_msg = String()
            action_msg.data = json.dumps(action_plan)
            self.action_pub.publish(action_msg)
        
        elif intent == 'move' and vl_result['action_needed']:
            # Use visual information to navigate safely
            location = entities.get('location', 'unknown location')
            
            action_plan = {
                'actions': [
                    {
                        'type': 'navigate_with_vision',
                        'target': location,
                        'visual_context': vl_result,
                        'description': f'Navigate to {location} using visual context'
                    }
                ]
            }
            
            # Publish action plan
            action_msg = String()
            action_msg.data = json.dumps(action_plan)
            self.action_pub.publish(action_msg)
```

## Speech Synthesis for Robot Responses

Generating natural-sounding speech responses for the robot:

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from std_msgs.msg import Bool
import pyttsx3
import threading
import queue

class SpeechSynthesisNode(Node):
    def __init__(self):
        super().__init__('speech_synthesis_node')
        
        # Subscribe to robot responses
        self.response_sub = self.create_subscription(
            String,
            '/robot_response',
            self.response_callback,
            10
        )
        
        # Publisher for speech completion
        self.speech_complete_pub = self.create_publisher(
            Bool,
            '/speech_complete',
            10
        )
        
        # Initialize text-to-speech engine
        self.tts_engine = pyttsx3.init()
        
        # Configure voice properties
        voices = self.tts_engine.getProperty('voices')
        if voices:
            # Use the first available voice (typically female voice on many systems)
            self.tts_engine.setProperty('voice', voices[0].id)
        
        # Set speech rate
        self.tts_engine.setProperty('rate', 150)  # Words per minute
        
        # Queue for speech requests
        self.speech_queue = queue.Queue()
        
        # Start speech synthesis thread
        self.speech_thread = threading.Thread(target=self.speech_worker)
        self.speech_thread.daemon = True
        self.speech_thread.start()
        
        self.get_logger().info("Speech synthesis node initialized")
    
    def response_callback(self, msg):
        """Add response to speech queue"""
        text = msg.data
        self.get_logger().info(f"Queuing speech: {text}")
        self.speech_queue.put(text)
    
    def speech_worker(self):
        """Process speech requests in a separate thread"""
        while rclpy.ok():
            try:
                # Wait for speech request (with timeout to allow checking rclpy.ok())
                text = self.speech_queue.get(timeout=1.0)
                
                if text:
                    self.get_logger().info(f"Speaking: {text}")
                    
                    # Speak the text
                    self.tts_engine.say(text)
                    self.tts_engine.runAndWait()
                    
                    # Publish completion message
                    complete_msg = Bool()
                    complete_msg.data = True
                    self.speech_complete_pub.publish(complete_msg)
                
                self.speech_queue.task_done()
            
            except queue.Empty:
                # Timeout occurred, continue loop to check rclpy.ok()
                continue
            except Exception as e:
                self.get_logger().error(f"Speech synthesis error: {str(e)}")

# Alternative using espeak for ROS 2 compatibility
class AlternativeSpeechSynthesisNode(Node):
    def __init__(self):
        super().__init__('alt_speech_synthesis')
        
        # Subscribe to robot responses
        self.response_sub = self.create_subscription(
            String,
            '/robot_response',
            self.response_callback,
            10
        )
        
        # Publisher for speech completion
        self.speech_complete_pub = self.create_publisher(
            Bool,
            '/speech_complete',
            10
        )
        
        self.get_logger().info("Alternative speech synthesis node initialized")
    
    def response_callback(self, msg):
        """Speak response using system TTS"""
        import subprocess
        
        text = msg.data
        self.get_logger().info(f"Speaking: {text}")
        
        try:
            # Use espeak or festival for text-to-speech
            # This is more compatible with ROS 2 systems
            subprocess.run(['espeak', '-s', '150', text], check=True)
            
            # Publish completion
            complete_msg = Bool()
            complete_msg.data = True
            self.speech_complete_pub.publish(complete_msg)
        
        except subprocess.CalledProcessError:
            self.get_logger().error("TTS command failed")
        except FileNotFoundError:
            self.get_logger().error("TTS command not found. Install espeak or festival.")
```

## Integration with Robot Actions

Connecting conversational AI with robot action execution:

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String, Bool
from sensor_msgs.msg import JointState
from trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint
from builtin_interfaces.msg import Duration
import json
import time

class ActionExecutorNode(Node):
    def __init__(self):
        super().__init__('action_executor')
        
        # Subscribe to action plans
        self.action_sub = self.create_subscription(
            String,
            '/robot_action',
            self.action_callback,
            10
        )
        
        # Publisher for joint commands
        self.joint_cmd_pub = self.create_publisher(
            JointTrajectory,
            '/joint_trajectory_controller/joint_trajectory',
            10
        )
        
        # Publisher for speech completion acknowledgment
        self.action_complete_pub = self.create_publisher(
            Bool,
            '/action_complete',
            10
        )
        
        # Store robot state
        self.joint_positions = {}
        
        # Subscribe to joint states
        self.joint_state_sub = self.create_subscription(
            JointState,
            '/joint_states',
            self.joint_state_callback,
            10
        )
        
        self.get_logger().info("Action executor node initialized")
    
    def joint_state_callback(self, msg):
        """Update joint positions from robot state"""
        for i, name in enumerate(msg.name):
            if i < len(msg.position):
                self.joint_positions[name] = msg.position[i]
    
    def action_callback(self, msg):
        """Execute action based on action plan"""
        try:
            action_data = json.loads(msg.data)
            action_type = action_data['type']
            
            self.get_logger().info(f"Executing action: {action_type}")
            
            if action_type == 'greet':
                self.execute_greet_action()
            elif action_type == 'navigate':
                target = action_data.get('target', 'unknown')
                self.execute_navigate_action(target)
            elif action_type == 'grasp_object':
                target = action_data.get('target', 'unknown')
                self.execute_grasp_action(target)
            elif action_type == 'place_object':
                obj = action_data.get('object', 'unknown')
                location = action_data.get('location', 'unknown')
                self.execute_place_action(obj, location)
            elif action_type == 'stop':
                self.execute_stop_action()
            else:
                self.get_logger().warn(f"Unknown action type: {action_type}")
        
        except json.JSONDecodeError:
            self.get_logger().error("Invalid JSON in action message")
    
    def execute_greet_action(self):
        """Execute greeting action - move head and arms"""
        # Move head to look at user
        self.move_head(0.0, 0.0)  # Look straight ahead
        
        # Move arms to greeting position (if robot has arms)
        self.move_arm_joints('left', [0.2, 0.5, 0.0, 0.0, 0.0, 0.0])
        self.move_arm_joints('right', [-0.2, 0.5, 0.0, 0.0, 0.0, 0.0])
        
        time.sleep(2)  # Hold greeting position
        
        # Return to neutral position
        self.move_arm_joints('left', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
        self.move_arm_joints('right', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
        
        # Publish completion
        self.publish_action_complete()
    
    def execute_navigate_action(self, target):
        """Execute navigation action"""
        # In a real implementation, this would interface with navigation stack
        # For simulation, we'll just log the action
        self.get_logger().info(f"Navigating to {target}")
        
        # Simulate navigation time
        time.sleep(3)
        
        # Publish completion
        self.publish_action_complete()
    
    def execute_grasp_action(self, target):
        """Execute grasping action"""
        self.get_logger().info(f"Attempting to grasp {target}")
        
        # Move arm to object location
        # This would involve complex IK and motion planning in practice
        self.move_arm_to_position('right', [0.5, 0.2, 0.8])  # Example coordinates
        
        # Close gripper
        self.close_gripper('right')
        
        # Lift object
        self.move_arm_to_position('right', [0.5, 0.2, 1.0])
        
        # Publish completion
        self.publish_action_complete()
    
    def execute_place_action(self, obj, location):
        """Execute placing action"""
        self.get_logger().info(f"Placing {obj} at {location}")
        
        # Move to placement location
        self.move_arm_to_position('right', [0.8, 0.0, 0.8])
        
        # Open gripper to release object
        self.open_gripper('right')
        
        # Move arm away
        self.move_arm_to_position('right', [0.5, 0.0, 1.0])
        
        # Publish completion
        self.publish_action_complete()
    
    def execute_stop_action(self):
        """Execute stop action"""
        self.get_logger().info("Stopping current action")
        
        # In a real implementation, this would stop all robot motion
        # For now, just log the action
        self.publish_action_complete()
    
    def move_head(self, pan, tilt):
        """Move robot head to specified angles"""
        # Publish head joint commands
        traj_msg = JointTrajectory()
        traj_msg.header.stamp = self.get_clock().now().to_msg()
        traj_msg.joint_names = ['head_pan_joint', 'head_tilt_joint']
        
        point = JointTrajectoryPoint()
        point.positions = [pan, tilt]
        point.time_from_start = Duration(sec=1, nanosec=0)
        traj_msg.points = [point]
        
        self.joint_cmd_pub.publish(traj_msg)
    
    def move_arm_joints(self, arm_side, joint_angles):
        """Move arm joints to specified angles"""
        # Determine joint names based on arm side
        if arm_side == 'left':
            joint_names = [
                'left_shoulder_yaw', 'left_shoulder_pitch', 
                'left_shoulder_roll', 'left_elbow_pitch',
                'left_wrist_pitch', 'left_wrist_roll'
            ]
        else:  # right
            joint_names = [
                'right_shoulder_yaw', 'right_shoulder_pitch', 
                'right_shoulder_roll', 'right_elbow_pitch',
                'right_wrist_pitch', 'right_wrist_roll'
            ]
        
        # Create trajectory message
        traj_msg = JointTrajectory()
        traj_msg.header.stamp = self.get_clock().now().to_msg()
        traj_msg.joint_names = joint_names
        
        point = JointTrajectoryPoint()
        point.positions = joint_angles
        point.time_from_start = Duration(sec=2, nanosec=0)  # 2 seconds to move
        traj_msg.points = [point]
        
        self.joint_cmd_pub.publish(traj_msg)
    
    def move_arm_to_position(self, arm_side, position):
        """Move arm to specific 3D position using IK"""
        # In practice, this would call an IK solver
        # For this example, we'll just move to a predefined joint configuration
        self.move_arm_joints(arm_side, [0.1, 0.5, 0.0, 0.5, 0.0, 0.0])
    
    def close_gripper(self, arm_side):
        """Close the gripper on specified arm"""
        # Publish gripper close command
        gripper_joint = f'{arm_side}_gripper_joint'
        
        traj_msg = JointTrajectory()
        traj_msg.header.stamp = self.get_clock().now().to_msg()
        traj_msg.joint_names = [gripper_joint]
        
        point = JointTrajectoryPoint()
        point.positions = [0.01]  # Closed position
        point.time_from_start = Duration(sec=1, nanosec=0)
        traj_msg.points = [point]
        
        self.joint_cmd_pub.publish(traj_msg)
    
    def open_gripper(self, arm_side):
        """Open the gripper on specified arm"""
        # Publish gripper open command
        gripper_joint = f'{arm_side}_gripper_joint'
        
        traj_msg = JointTrajectory()
        traj_msg.header.stamp = self.get_clock().now().to_msg()
        traj_msg.joint_names = [gripper_joint]
        
        point = JointTrajectoryPoint()
        point.positions = [0.05]  # Open position
        point.time_from_start = Duration(sec=1, nanosec=0)
        traj_msg.points = [point]
        
        self.joint_cmd_pub.publish(traj_msg)
    
    def publish_action_complete(self):
        """Publish action completion message"""
        complete_msg = Bool()
        complete_msg.data = True
        self.action_complete_pub.publish(complete_msg)

class ConversationalRobotNode(Node):
    def __init__(self):
        super().__init__('conversational_robot')
        
        # Publisher for system status
        self.status_pub = self.create_publisher(
            String,
            '/system_status',
            10
        )
        
        # Timer for system monitoring
        self.status_timer = self.create_timer(5.0, self.publish_status)
        
        self.get_logger().info("Conversational robot system initialized")
    
    def publish_status(self):
        """Publish system status"""
        status_msg = String()
        status_msg.data = "Conversational AI system operational"
        self.status_pub.publish(status_msg)
```

## System Integration and Launch

A launch file to bring up the entire conversational AI system:

```python
# launch/conversational_ai.launch.py
from launch import LaunchDescription
from launch_ros.actions import Node

def generate_launch_description():
    return LaunchDescription([
        # Speech recognition node
        Node(
            package='conversational_ai',
            executable='speech_recognition_node',
            name='speech_recognition',
            parameters=[
                {'model_size': 'base'}
            ],
            output='screen'
        ),
        
        # Natural language understanding node
        Node(
            package='conversational_ai',
            executable='nlu_node',
            name='nlu',
            output='screen'
        ),
        
        # Dialogue manager node
        Node(
            package='conversational_ai',
            executable='dialogue_manager',
            name='dialogue_manager',
            output='screen'
        ),
        
        # Vision-language node
        Node(
            package='conversational_ai',
            executable='vision_language_node',
            name='vision_language',
            output='screen'
        ),
        
        # Speech synthesis node
        Node(
            package='conversational_ai',
            executable='speech_synthesis_node',
            name='speech_synthesis',
            output='screen'
        ),
        
        # Action executor node
        Node(
            package='conversational_ai',
            executable='action_executor_node',
            name='action_executor',
            output='screen'
        ),
        
        # Main system node
        Node(
            package='conversational_ai',
            executable='conversational_robot_node',
            name='conversational_robot',
            output='screen'
        )
    ])
```

## Evaluation and Testing

Evaluating the effectiveness of conversational AI systems:

```python
class ConversationalAIEvaluator(Node):
    def __init__(self):
        super().__init__('conversational_ai_evaluator')
        
        # Subscribe to system events
        self.speech_rec_sub = self.create_subscription(
            String,
            '/recognized_speech',
            self.speech_rec_callback,
            10
        )
        
        self.intent_sub = self.create_subscription(
            String,
            '/intent',
            self.intent_callback,
            10
        )
        
        self.response_sub = self.create_subscription(
            String,
            '/robot_response',
            self.response_callback,
            10
        )
        
        self.action_sub = self.create_subscription(
            String,
            '/robot_action',
            self.action_callback,
            10
        )
        
        # Timer for periodic evaluation
        self.eval_timer = self.create_timer(10.0, self.evaluate_performance)
        
        # Metrics storage
        self.metrics = {
            'total_interactions': 0,
            'successful_interactions': 0,
            'average_response_time': 0.0,
            'intent_accuracy': 0.0,
            'user_satisfaction': 0.0
        }
        
        self.interaction_start_time = None
        
        self.get_logger().info("Conversational AI evaluator initialized")
    
    def speech_rec_callback(self, msg):
        """Track speech recognition events"""
        self.interaction_start_time = self.get_clock().now()
        self.metrics['total_interactions'] += 1
    
    def intent_callback(self, msg):
        """Track intent recognition"""
        pass  # Intent recognition tracked separately
    
    def response_callback(self, msg):
        """Track response generation"""
        if self.interaction_start_time:
            response_time = (self.get_clock().now() - self.interaction_start_time).nanoseconds / 1e9
            self.metrics['average_response_time'] = response_time
            self.interaction_start_time = None  # Reset for next interaction
    
    def action_callback(self, msg):
        """Track action execution"""
        self.metrics['successful_interactions'] += 1
    
    def evaluate_performance(self):
        """Evaluate and log system performance"""
        total = self.metrics['total_interactions']
        successful = self.metrics['successful_interactions']
        
        if total > 0:
            success_rate = successful / total
            self.metrics['intent_accuracy'] = success_rate
            
            self.get_logger().info(
                f"Conversational AI Performance:\n"
                f"  Total interactions: {total}\n"
                f"  Successful interactions: {successful}\n"
                f"  Success rate: {success_rate:.2f}\n"
                f"  Average response time: {self.metrics['average_response_time']:.2f}s"
            )
        
        # Reset metrics periodically
        self.metrics['total_interactions'] = 0
        self.metrics['successful_interactions'] = 0
```

## Key Challenges and Practical Insights

Implementing conversational AI for humanoid robots presents several challenges:

1. **Real-time Performance**: All components must operate in real-time for natural interaction
2. **Robustness**: Systems must handle noisy environments and varied speech patterns
3. **Context Management**: Maintaining conversation context across multiple turns
4. **Multimodal Integration**: Coordinating speech, vision, and action seamlessly
5. **Safety**: Ensuring robot actions based on voice commands are safe
6. **Privacy**: Handling sensitive information appropriately

## Looking Ahead

This completes our exploration of Physical AI and Humanoid Robotics. You now have a comprehensive understanding of how to develop AI systems that function in the physical world, from middleware and simulation to perception, manipulation, and natural interaction. The future of AI extends beyond digital spaces into the physical world, and you're now equipped to contribute to this exciting field.